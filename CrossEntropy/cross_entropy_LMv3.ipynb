{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c5e8ea-8f52-46b9-8dfa-bfebfd50db41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # Set the default device to GPU\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f4fb88-6c41-4c32-9340-eeedf5df834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# !pip install transformers datasets torch\n",
    "#!pip install imgaug\n",
    "#!pip install seqeval\n",
    "#!pip install transformers[torch]\n",
    "# !pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0f4264-f6db-4cec-9630-9515e17fdaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makni\\anaconda3\\envs\\PFA\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification,AutoProcessor\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import tqdm as notebook_tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import fitz  # PyMuPDF\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from skimage import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_metric\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9b54ec-bf9d-455e-8b65-6fdf8190455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotation_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5322af-0301-4f31-8f6f-232cdbbde01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation_data= read_annotation_file(\"C:/Users/makni/Downloads/ieee/preprocessed_annotations/WRO-1.pdf.json\")\n",
    "\n",
    "# # annotation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82376b36-a3cd-4464-b1ba-6ea660d94d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageDraw\n",
    "# import json\n",
    "\n",
    "# def display_image_with_annotations(image_path, json_path, bbox_color='red'):\n",
    "#     # Load the image and convert to RGB\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     print(image.size)\n",
    "#     draw = ImageDraw.Draw(image)\n",
    "    \n",
    "#     # Load the annotation data\n",
    "#     with open(json_path, 'r') as file:\n",
    "#         annotations = json.load(file)\n",
    "    \n",
    "#     # Iterate through the annotations and draw rectangles\n",
    "#     for word, bbox in zip(annotations['words'], annotations['bboxes']):\n",
    "#         draw.rectangle([bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]], outline=bbox_color, width=2)\n",
    "#         draw.text((bbox[0] + 2, bbox[1] + 2), word, fill='blue')\n",
    "    \n",
    "#     # Display the image\n",
    "#     image.show()\n",
    "\n",
    "\n",
    "# # Test the function\n",
    "# annotation_path = \"C:/Users/makni/Downloads/ieee/preprocessed_annotations/20231217_190133_merged-10.pdf.json\"\n",
    "# image_path = \"C:/Users/makni/Downloads/ieee/preprocessed_images/20231217_190133_merged-10.pdf_1.png\"\n",
    "# display_image_with_annotations(image_path, annotation_path)  # Change bbox_color to any color you like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234db64c-7e65-4bec-9f55-89493a2b7519",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4ad3f6-8c93-4fa5-9eaa-11dfa95f8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.MedianBlur(blur_limit=3, p=0.3),\n",
    "        A.ColorJitter(p=0.3),\n",
    "        A.PadIfNeeded(min_height=1000, min_width=700, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], p=1.0),\n",
    "        A.Rotate(limit=5, p=1, interpolation=cv2.INTER_CUBIC),\n",
    "    ], bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        label_fields=['labels'],\n",
    "        min_area=0,\n",
    "        min_visibility=0.1  \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86e8935d-47a0-4208-a0fc-a10b7ff28910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccc908-4e90-4660-82ba-632c74e07769",
   "metadata": {},
   "source": [
    "# LayoutLmv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ebf97-4565-4ed8-bc71-618c6aceda5b",
   "metadata": {},
   "source": [
    "## Label Dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2251e8ef-afb9-449d-8a69-39f987831ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'O': 0,\n",
    "    'B-title': 1,\n",
    "    'I-title': 2,\n",
    "    'B-date': 3,\n",
    "    'I-date': 4,\n",
    "    'B-ieee': 5,\n",
    "    'I-ieee': 6,\n",
    "    'B-total': 7,\n",
    "    'I-total': 8,\n",
    "    'B-totalValue': 9,\n",
    "    'I-totalValue': 10\n",
    "}\n",
    "\n",
    "# Create id2label by reversing label2id\n",
    "id2label = {id: label for label, id in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd8dc6-30a1-4c32-8424-7f5849d4a18a",
   "metadata": {},
   "source": [
    "## Processor Initializationng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d29b41b-eb3e-4728-ba63-0af32b263f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0cacd-1111-454e-895c-5a4a4fafa7f8",
   "metadata": {},
   "source": [
    "## Custom Dataset Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a9f3e0-0076-4d12-8183-80a729475d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop_to_bounding_boxes(image_np, boxes):\n",
    "#         if not boxes:\n",
    "#             return image_np  # Return original if no boxes\n",
    "    \n",
    "#         # Initialize min and max coordinates with values from the first box\n",
    "#         x_min = boxes[0][0]\n",
    "#         y_min = boxes[0][1]\n",
    "#         x_max = boxes[0][2]\n",
    "#         y_max = boxes[0][3]\n",
    "    \n",
    "#         # Iterate over all boxes to find the minimum and maximum extents\n",
    "#         for box in boxes:\n",
    "#             x_min = min(x_min, box[0])\n",
    "#             y_min = min(y_min, box[1])\n",
    "#             x_max = max(x_max, box[2])\n",
    "#             y_max = max(y_max, box[3])\n",
    "    \n",
    "#         # Crop the image using the min and max coordinates\n",
    "#         image_cropped = image_np[y_min-20:y_max+20, x_min-20:x_max+20]\n",
    "    \n",
    "#         adjusted_boxes = []\n",
    "#         for box in boxes:\n",
    "#             adjusted_box = [\n",
    "#                 box[0] - x_min+20 ,\n",
    "#                 box[1] - y_min+20,\n",
    "#                 box[2] - x_min+20,\n",
    "#                 box[3] - y_min+20\n",
    "#             ]\n",
    "#             adjusted_boxes.append(adjusted_box)\n",
    "    \n",
    "#         return image_cropped, adjusted_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7076c69-2e5c-4c9c-bba6-fef88d49d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, annotation_dir, image_dir, annotation_files, transforms=None):\n",
    "#         self.annotations = [os.path.join(annotation_dir, file) for file in annotation_files]\n",
    "#         self.image_dir = image_dir\n",
    "#         self.processor = processor  \n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         annotation_path = self.annotations[idx]\n",
    "#         with open(annotation_path, 'r') as f:\n",
    "#             data = json.load(f)\n",
    "\n",
    "#         image_path = os.path.join(self.image_dir, os.path.basename(data[\"path\"]))\n",
    "#         image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "#         width, height = image.size\n",
    "\n",
    "#         words = data[\"words\"]\n",
    "#         labels = [label2id[label] for label in data[\"ner_tags\"]]\n",
    "#         boxes = data[\"bboxes\"]\n",
    "\n",
    "#         image_np = np.array(image)  # Convert PIL Image to NumPy array for transformation\n",
    "#         # print(f\"Before transformation: {len(words)} words, {len(boxes)} boxes\")\n",
    "\n",
    "#         if self.transforms:\n",
    "#             transformed = self.transforms(image=image_np, bboxes=boxes, labels=labels)\n",
    "#             image_np = transformed['image']\n",
    "#             boxes = transformed['bboxes']\n",
    "#             labels = transformed['labels']\n",
    "            \n",
    "#         boxes = [[int(box[0]), int(box[1]), int(box[2]), int(box[3] )] for box in boxes]\n",
    "#         cropped_image, boxes = crop_to_bounding_boxes(image_np, boxes)\n",
    "\n",
    "#         image = Image.fromarray(cropped_image)  \n",
    "#         image = image.resize((600, 900))\n",
    "#         width, height = image.size\n",
    "        \n",
    "#         boxes = [[bbox[0] / width, bbox[1] / height, bbox[2] / width, bbox[3] / height] for bbox in boxes]\n",
    "#         boxes = [[int(box[0] * 1000), int(box[1] * 1000), int(box[2] * 1000), int(box[3] * 1000)] for box in boxes]\n",
    "\n",
    "#         image.convert(\"RGB\")\n",
    "#         encoding = self.processor(image, words, boxes=boxes, word_labels=labels, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "#         bbox_tensor = torch.tensor(boxes, dtype=torch.long)\n",
    "#         label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "#         return {\n",
    "#             'words': words,\n",
    "#             'input_ids': encoding['input_ids'].squeeze(0),\n",
    "#             'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "#             'bbox': bbox_tensor,  \n",
    "#             'labels': label_tensor\n",
    "#         }\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "\n",
    "#     def check_and_adjust_boxes(self, boxes, img_width, img_height):\n",
    "#         adjusted_boxes = []\n",
    "#         for box in boxes:\n",
    "#             x_min, y_min, x_max, y_max = box\n",
    "#             if x_min < 0 or y_min < 0 or x_max > img_width or y_max > img_height:\n",
    "#                 # Box is out of bounds, adjust or discard\n",
    "#                 x_min = max(0, x_min)\n",
    "#                 y_min = max(0, y_min)\n",
    "#                 x_max = min(img_width, x_max)\n",
    "#                 y_max = min(img_height, y_max)\n",
    "#             adjusted_boxes.append([x_min, y_min, x_max, y_max])\n",
    "#         return adjusted_boxes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ed778ef-0713-4898-970e-e8f11c1cead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import clamp\n",
    "\n",
    "def crop_to_bounding_boxes(image_np, boxes):\n",
    "        if not boxes:\n",
    "            return image_np  # Return original if no boxes\n",
    "    \n",
    "        # Initialize min and max coordinates with values from the first box\n",
    "        x_min = boxes[0][0]\n",
    "        y_min = boxes[0][1]\n",
    "        x_max = boxes[0][2]\n",
    "        y_max = boxes[0][3]\n",
    "    \n",
    "        # Iterate over all boxes to find the minimum and maximum extents\n",
    "        for box in boxes:\n",
    "            x_min = min(x_min, box[0])\n",
    "            y_min = min(y_min, box[1])\n",
    "            x_max = max(x_max, box[2])\n",
    "            y_max = max(y_max, box[3])\n",
    "    \n",
    "        # Crop the image using the min and max coordinates\n",
    "        image_cropped = image_np[y_min-20:y_max+20, x_min-20:x_max+20]\n",
    "    \n",
    "        adjusted_boxes = []\n",
    "        for box in boxes:\n",
    "            adjusted_box = [\n",
    "                box[0] - x_min+20 ,\n",
    "                box[1] - y_min+20,\n",
    "                box[2] - x_min+20,\n",
    "                box[3] - y_min+20\n",
    "            ]\n",
    "            adjusted_boxes.append(adjusted_box)\n",
    "    \n",
    "        return image_cropped, adjusted_boxes\n",
    "def clamp_values(value, min_value, max_value):\n",
    "    return max(min_value, min(value, max_value))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotation_dir, image_dir, annotation_files, transforms=None):\n",
    "        self.annotations = [os.path.join(annotation_dir, file) for file in annotation_files]\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor  \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation_path = self.annotations[idx]\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, os.path.basename(data[\"path\"]))\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        words = data[\"words\"]\n",
    "        labels = [label2id[label] for label in data[\"ner_tags\"]]\n",
    "        boxes = data[\"bboxes\"]\n",
    "\n",
    "        image_np = np.array(image)  # Convert PIL Image to NumPy array for transformation\n",
    "        # print(f\"Before transformation: {len(words)} words, {len(boxes)} boxes\")\n",
    "\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image=image_np, bboxes=boxes, labels=labels)\n",
    "            image_np = transformed['image']\n",
    "            boxes = transformed['bboxes']\n",
    "            labels = transformed['labels']\n",
    "\n",
    "\n",
    "        boxes = [[int(box[0]), int(box[1]), int(box[2]), int(box[3] )] for box in boxes]\n",
    "        cropped_image, boxes = crop_to_bounding_boxes(image_np, boxes)\n",
    "\n",
    "        boxes = [[int(box[0]), int(box[1]), int(box[2]), int(box[3] )] for box in boxes]\n",
    "\n",
    "\n",
    "        image = Image.fromarray(cropped_image)  \n",
    "        image = image.resize((600, 900), resample=Image.BILINEAR)\n",
    "        # boxes = normalize_bboxes(boxes, 600, 900)\n",
    "        # boxes = [[int(box[0]), int(box[1]), int(box[2]), int(box[3] )] for box in boxes]\n",
    "\n",
    "        boxes = [[bbox[0] / width, bbox[1] / height, bbox[2] / width, bbox[3] / height] for bbox in boxes]\n",
    "        boxes = [[int(box[0] * 1000), int(box[1] * 1000), int(box[2] * 1000), int(box[3] * 1000)] for box in boxes]\n",
    "        \n",
    "        boxes = [[clamp_values(coord, 0, 1000) for coord in box] for box in boxes]\n",
    "\n",
    "        image.convert(\"RGB\")\n",
    "        encoding = self.processor(image, words, boxes=boxes, word_labels=labels, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        bbox_tensor = torch.tensor(boxes, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'words': words,\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'bbox': bbox_tensor,  \n",
    "            'labels': label_tensor\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df715a0-a56a-4656-a1c0-058eedbaf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "annotation_dir = \"output/\"\n",
    "image_dir = \"resized_images/\" \n",
    "annotation_files = [file for file in os.listdir(annotation_dir) if file.endswith('.json')]\n",
    "np.random.shuffle(annotation_files)  \n",
    "split_idx = int(0.8 * len(annotation_files))\n",
    "train_files = annotation_files[:split_idx]\n",
    "test_files = annotation_files[split_idx:]\n",
    "train_dataset = CustomDataset(annotation_dir, image_dir, annotation_files=train_files,transforms=get_train_transform()) #,transforms=get_train_transform()\n",
    "test_dataset = CustomDataset(annotation_dir, image_dir, annotation_files=test_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc0c114-0b4e-4f29-aec7-2ff483ba284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range (1,156):\n",
    "#     # print(i)\n",
    "example = train_dataset[1]\n",
    "# print(example)\n",
    "# image= example[\"image\"]\n",
    "# print(image)\n",
    "words = example[\"words\"]\n",
    "boxes = example[\"bbox\"]\n",
    "ner_tags = example[\"labels\"]\n",
    "    # ids= example[\"input_ids\"]\n",
    "    # masks= example[\"attention_mask\"]\n",
    "    # print(\"masks:\", masks)\n",
    "    # print(\"ids:\", ids)\n",
    "    \n",
    "    # print(len(words),boxes.size() ,ner_tags.size() )\n",
    "    # print(\"Bounding Boxes:\", boxes.size())\n",
    "    # # print(\"Bounding Boxes:\", boxes)\n",
    "    # print(\"NER Tags:\", ner_tags.size())\n",
    "    # print(boxes.shape)\n",
    "    # print(ner_tags.shape)\n",
    "    # print(ner_tags)\n",
    "    # print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40a48501-1b54-40ac-9d10-063f2b33d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    max_len = max([len(item['input_ids']) for item in batch])\n",
    "    input_ids = torch.stack([torch.cat([item['input_ids'], torch.tensor([processor.tokenizer.pad_token_id] * (max_len - len(item['input_ids'])), dtype=torch.long)]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.cat([item['attention_mask'], torch.tensor([0] * (max_len - len(item['attention_mask'])), dtype=torch.long)]) for item in batch])\n",
    "\n",
    "    bbox_padded = []\n",
    "    for item in batch:\n",
    "        padding_length = max_len - item['bbox'].shape[0]\n",
    "        padded_bbox = torch.cat([item['bbox'], torch.zeros(padding_length, 4, dtype=torch.int64)])\n",
    "        bbox_padded.append(padded_bbox)\n",
    "    bbox = torch.stack(bbox_padded)\n",
    "\n",
    "    labels = torch.stack([torch.cat([item['labels'], torch.tensor([-100] * (max_len - len(item['labels'])), dtype=torch.long)]) for item in batch])\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'bbox': bbox,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "542c4fff-1e5c-492e-b6e7-0aaaaeabf5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([4, 512]), 'attention_mask': torch.Size([4, 512]), 'bbox': torch.Size([4, 512, 4]), 'labels': torch.Size([4, 512])}\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000026E9534B850>\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    break \n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6153f3d-a038-4f00-8291-7bf6b58c5e33",
   "metadata": {},
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8893e7-45b4-40b1-a29a-147b2106e233",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93bdaa49-15c9-4d29-8e6f-de5921758813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c21b447d-dae4-4a87-9c35-b61cd2e8a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# deprecated \n",
    "# from datasets import load_metric\n",
    "# metric = load_metric(\"seqeval\")\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "label_list = list(model.config.id2label.values())\n",
    "\n",
    "\n",
    "def compute_metrics(p, return_entity_level_metrics=False):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Set zero_division=1 to handle classes with no predictions gracefully\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=1)\n",
    "    if return_entity_level_metrics:\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f955f712-8f8b-4a21-9aab-0506206a9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LayoutLMv3FeatureExtractor, AutoTokenizer, LayoutLMv3Processor\n",
    "# from transformers import LiltForTokenClassification\n",
    "# # huggingface hub model id\n",
    "# model_id = \"SCUT-DLVCLab/lilt-roberta-en-base\"\n",
    "# # load model with correct number of labels and mapping\n",
    "# model = LiltForTokenClassification.from_pretrained(\n",
    "#     model_id, num_labels=11, label2id=label2id, id2label=id2label\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c888d7-f44d-4267-ae1b-52b8e91f2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Utilisez cette ligne pour créer l'optimiseur\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-3, no_deprecation_warning=True)\n",
    "# # Move the model to the GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee988630-d088-4b0b-84d8-d8baeed5adf1",
   "metadata": {},
   "source": [
    "# Define TrainingArguments + Trainer\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd5818c4-0914-4783-8ed9-1e4d7deb0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a62dd5-6fc6-41ae-8da5-75a1811245e9",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f2bf89d-78aa-4ae5-9fc9-13d5bd68a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'PFA-mlflow' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/akramLh005/PFA-mlflow.git\n",
    "!cd PFA-mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49f137cd-afaf-46f8-bb85-c5216f1a5c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow==2.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d4036a8-b54a-4fd2-b036-b64f668bacfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_URI=https://dagshub.com/akramLh005/PFA-mlflow.mlflow\n",
      "env: MLFLOW_TRACKING_USERNAME=akramLh005\n",
      "env: MLFLOW_TRACKING_PASSWORD=fabaf87fce39ef7e9bfdd406d6be4354ce2a25f4\n"
     ]
    }
   ],
   "source": [
    "%env MLFLOW_TRACKING_URI=https://dagshub.com/akramLh005/PFA-mlflow.mlflow\n",
    "%env MLFLOW_TRACKING_USERNAME=akramLh005\n",
    "%env MLFLOW_TRACKING_PASSWORD=fabaf87fce39ef7e9bfdd406d6be4354ce2a25f4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d301ca-d239-4760-a6d7-623fca4d4d15",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6cd6598-c687-46e4-aca2-6036363d6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cf5903c-e8d1-4428-a2e5-e2ae02f0a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def check_gradient(model):\n",
    "    def hook(grad):\n",
    "        if torch.isnan(grad).any() or torch.isinf(grad).any():\n",
    "            print(\"NaN or Inf in gradients.\")\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.register_hook(hook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acef920b-318f-426a-a28a-19db4bc4c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comptages de classes : [25452, 134, 35, 147, 53, 128, 290, 149, 97, 145, 28]\n"
     ]
    }
   ],
   "source": [
    "def calculate_class_counts(dataset):\n",
    "    # Initialiser un dictionnaire pour compter les occurrences de chaque classe\n",
    "    class_counts = {label: 0 for label in label2id.values()}\n",
    "\n",
    "    # Parcourir le jeu de données\n",
    "    for i in range(len(dataset)):\n",
    "        sample = dataset[i]\n",
    "        labels = sample['labels'].numpy()  # Assurez-vous que les labels sont sous forme de Numpy array pour comptage\n",
    "        \n",
    "        # Compter chaque label\n",
    "        for label in labels:\n",
    "            if label != -100:  # Assurez-vous de ne pas compter les labels de padding\n",
    "                class_counts[label] += 1\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Appliquez cette fonction à votre jeu de données d'entraînement\n",
    "class_counts = calculate_class_counts(train_dataset)\n",
    "class_counts_list = [class_counts[i] for i in range(len(label2id))]  # Transformer en liste pour l'usage ultérieur\n",
    "\n",
    "print(\"Comptages de classes :\", class_counts_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33d7f5c5-34ea-4308-a5d0-e84879749a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids pour chaque classe : [1.047443148621554, 198.70071542330544, 680.5498298625424, 186.45204202383275, 697.9998210256869, 204.6766763400995, 86.14556689412764, 181.47998790133414, 274.9696691949829, 185.18366087185981, 1046.9995973078471]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForTokenClassification(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = [25989, 137, 40, 146, 39, 133, 316, 150, 99, 147, 26]  # Comptages fournis\n",
    "total_count = sum(class_counts)\n",
    "class_weights = [total_count / count for count in class_counts]\n",
    "\n",
    "# Convertir en tensor de poids pour PyTorch\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "total_count = sum(class_counts)\n",
    "class_weights = [total_count / (count + 1e-5) for count in class_counts]  # Small epsilon to avoid division by zero\n",
    "\n",
    "\n",
    "# Afficher les poids calculés pour chaque classe\n",
    "print(\"Poids pour chaque classe :\", class_weights)\n",
    "from torch import nn\n",
    "device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4e183dc-a2ba-4fe0-95c1-f13e4f108015",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c124238-5f1a-4938-8292-b800353c95f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x26e94b78ca0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "327ba505-3aa2-4a20-8fb6-84ecd0146815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayoutLMv3ForTokenClassification(\n",
       "  (layoutlmv3): LayoutLMv3Model(\n",
       "    (embeddings): LayoutLMv3TextEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (x_position_embeddings): Embedding(1024, 128)\n",
       "      (y_position_embeddings): Embedding(1024, 128)\n",
       "      (h_position_embeddings): Embedding(1024, 128)\n",
       "      (w_position_embeddings): Embedding(1024, 128)\n",
       "    )\n",
       "    (patch_embed): LayoutLMv3PatchEmbeddings(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (encoder): LayoutLMv3Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x LayoutLMv3Layer(\n",
       "          (attention): LayoutLMv3Attention(\n",
       "            (self): LayoutLMv3SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): LayoutLMv3SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LayoutLMv3Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): LayoutLMv3Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_pos_bias): Linear(in_features=32, out_features=12, bias=False)\n",
       "      (rel_pos_x_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "      (rel_pos_y_bias): Linear(in_features=64, out_features=12, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): LayoutLMv3ClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28c55cc0-1f6a-4be8-aaa6-0654e0d5367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['classifier.bias', 'classifier.weight'].\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "loaded state dict contains a parameter group that doesn't match the size of optimizer's group",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m training_args\u001b[38;5;241m.\u001b[39mlr_scheduler_num_warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m num_training_steps)  \n\u001b[0;32m     96\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mstart_run()\n\u001b[1;32m---> 97\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mend_run()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\transformers\\trainer.py:1848\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1849\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\transformers\\trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_from_checkpoint(resume_from_checkpoint, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped)\n\u001b[0;32m   2028\u001b[0m \u001b[38;5;66;03m# Check if saved optimizer or scheduler states exist\u001b[39;00m\n\u001b[1;32m-> 2029\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;66;03m# important: at this point:\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;66;03m# self.model         is the Transformers Model\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \u001b[38;5;66;03m# self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model),\u001b[39;00m\n\u001b[0;32m   2034\u001b[0m \u001b[38;5;66;03m# FSDP(Transformers Model), Dynamo Optimized Module(Transformers Model) etc.\u001b[39;00m\n\u001b[0;32m   2035\u001b[0m \n\u001b[0;32m   2036\u001b[0m \u001b[38;5;66;03m# Train!\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m***** Running training *****\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\transformers\\trainer.py:2944\u001b[0m, in \u001b[0;36mTrainer._load_optimizer_and_scheduler\u001b[1;34m(self, checkpoint)\u001b[0m\n\u001b[0;32m   2935\u001b[0m         load_fsdp_optimizer(\n\u001b[0;32m   2936\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin,\n\u001b[0;32m   2937\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2941\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_get_fsdp_ckpt_kwargs(),\n\u001b[0;32m   2942\u001b[0m         )\n\u001b[0;32m   2943\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2944\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2946\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2947\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m caught_warnings:\n\u001b[0;32m   2948\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint, SCHEDULER_NAME)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\accelerate\\optimizer.py:107\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.load_state_dict\u001b[1;34m(self, state_dict)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_placement:\n\u001b[0;32m    106\u001b[0m     xm\u001b[38;5;241m.\u001b[39msend_cpu_data_to_device(state_dict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\PFA\\lib\\site-packages\\torch\\optim\\optimizer.py:747\u001b[0m, in \u001b[0;36mOptimizer.load_state_dict\u001b[1;34m(self, state_dict)\u001b[0m\n\u001b[0;32m    745\u001b[0m saved_lens \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m saved_groups)\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(p_len \u001b[38;5;241m!=\u001b[39m s_len \u001b[38;5;28;01mfor\u001b[39;00m p_len, s_len \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(param_lens, saved_lens)):\n\u001b[1;32m--> 747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded state dict contains a parameter group \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    748\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the size of optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms group\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# Update the state\u001b[39;00m\n\u001b[0;32m    751\u001b[0m id_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chain\u001b[38;5;241m.\u001b[39mfrom_iterable(g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m saved_groups),\n\u001b[0;32m    752\u001b[0m               chain\u001b[38;5;241m.\u001b[39mfrom_iterable(g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups)))\n",
      "\u001b[1;31mValueError\u001b[0m: loaded state dict contains a parameter group that doesn't match the size of optimizer's group"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AdamW, get_scheduler\n",
    "import mlflow\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from torch.optim import AdamW \n",
    "\n",
    "weight_decay_rate = 0.01\n",
    "mlflow.end_run()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    max_grad_norm=1.0, \n",
    "    \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "\n",
    "    \n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=50,\n",
    "    learning_rate=5e-5,\n",
    "\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,\n",
    "\n",
    "    weight_decay = 0.01,\n",
    "\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    max_steps=2000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "class MLflowTrainer(Trainer):\n",
    "    def __init__(self, class_weights_tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_func = nn.CrossEntropyLoss(weight=class_weights_tensor, ignore_index=-100)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        loss = self.loss_func(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with self.autocast_smart_context_manager():\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss /= self.args.gradient_accumulation_steps\n",
    "        self.accelerator.backward(loss)\n",
    "        if self.args.max_grad_norm is not None:\n",
    "            clip_grad_norm_(model.parameters(), self.args.max_grad_norm)\n",
    "        logs = {\"loss\": loss.item()}\n",
    "        self.log(logs)\n",
    "        return loss.detach()\n",
    "\n",
    "    def log(self, logs):\n",
    "        super().log(logs)\n",
    "        for key, value in logs.items():\n",
    "            mlflow.log_metric(key, value, step=self.state.global_step)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        output = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        for key, value in output.items():\n",
    "            mlflow.log_metric(f\"{metric_key_prefix}_{key}\", value, step=self.state.global_step)\n",
    "\n",
    "trainer = MLflowTrainer(\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=custom_collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4, weight_decay=weight_decay_rate)\n",
    "num_training_steps = training_args.max_steps\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps), \n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "training_args.lr_scheduler_type = 'linear'\n",
    "training_args.lr_scheduler_num_warmup_steps = int(0.1 * num_training_steps)  \n",
    "mlflow.start_run()\n",
    "trainer.train(resume_from_checkpoint = True)\n",
    "mlflow.end_run()\n",
    "\n",
    "\n",
    "# Step\tTraining Loss\tValidation Loss\tPrecision\tRecall\tF1\tAccuracy\n",
    "# 50\t2.172701\t1.608711\t0.014266\t0.263959\t0.027069\t0.087081\n",
    "# 100\t1.220003\t1.293408\t0.021686\t0.451777\t0.041386\t0.154461\n",
    "# 150\t0.910068\t1.133811\t0.031823\t0.563452\t0.060244\t0.340043\n",
    "# 200\t1.418300\t1.130773\t0.034183\t0.598985\t0.064675\t0.366310\n",
    "# 250\t0.557900\t1.044300\t0.038925\t0.573604\t0.072903\t0.469665\n",
    "# 300\t0.699392\t1.105067\t0.056467\t0.593909\t0.103129\t0.639971\n",
    "# 350\t0.343031\t1.190133\t0.057806\t0.593909\t0.105358\t0.632976\n",
    "# 400\t0.493500\t1.224606\t0.072629\t0.614213\t0.129898\t0.736759\n",
    "# 450\t0.281491\t1.672526\t0.118902\t0.593909\t0.198137\t0.842684\n",
    "# 500\t0.203167\t1.508098\t0.094281\t0.619289\t0.163649\t0.771592\n",
    "# 550\t0.153609\t1.705901\t0.175940\t0.593909\t0.271462\t0.892791\n",
    "# 600\t0.226000\t1.819784\t0.128062\t0.583756\t0.210046\t0.865525\n",
    "# 650\t0.045874\t1.937642\t0.176287\t0.573604\t0.269690\t0.895503\n",
    "# 700\t0.026339\t1.973493\t0.226580\t0.527919\t0.317073\t0.930050\n",
    "# 750\t0.040032\t2.068556\t0.269767\t0.588832\t0.370016\t0.936474\n",
    "# 800\t0.129300\t2.128340\t0.223062\t0.598985\t0.325069\t0.919914\n",
    "# 850\t0.053644\t2.254380\t0.247951\t0.614213\t0.353285\t0.930193\n",
    "# 900\t0.055653\t2.292882\t0.252700\t0.593909\t0.354545\t0.933476\n",
    "# 950\t0.046442\t2.588771\t0.280193\t0.588832\t0.379705\t0.941899\n",
    "# 1000\t0.085500\t2.407752\t0.290960\t0.522843\t0.373866\t0.945182\n",
    "# 1050\t0.265607\t2.692389\t0.400000\t0.568528\t0.469602\t0.959315\n",
    "# 1100\t0.007815\t2.818417\t0.382022\t0.517766\t0.439655\t0.958458\n",
    "# 1150\t0.009128\t3.011377\t0.399240\t0.532995\t0.456522\t0.959743\n",
    "# 1200\t0.053700\t2.781135\t0.391753\t0.578680\t0.467213\t0.958458\n",
    "# 1250\t0.010240\t2.894034\t0.374101\t0.527919\t0.437895\t0.959029\n",
    "# 1300\t0.106429\t3.191725\t0.481481\t0.527919\t0.503632\t0.965882\n",
    "# 1350\t0.062707\t3.025777\t0.440476\t0.563452\t0.494432\t0.962027\n",
    "# 1400\t0.041300\t3.192157\t0.464286\t0.527919\t0.494062\t0.965168\n",
    "# 1450\t0.005224\t3.240162\t0.448718\t0.532995\t0.487239\t0.963740\n",
    "# 1500\t0.004589\t3.366700\t0.459459\t0.517766\t0.486874\t0.965168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8d08e-8503-4a12-b231-160005289c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TrainingArguments, Trainer, AdamW, get_scheduler\n",
    "# import torch\n",
    "# import mlflow\n",
    "# from transformers import EarlyStoppingCallback\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# mlflow.end_run()\n",
    "\n",
    "# weight_decay_rate = 0.01\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"test_trainer\",\n",
    "#     max_grad_norm=1.0, \n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=50,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=200,\n",
    "#     save_total_limit=2,\n",
    "#     num_train_epochs=50,\n",
    "#     learning_rate=5e-4,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     fp16=True,\n",
    "#     weight_decay=weight_decay_rate,\n",
    "#     max_steps=1800,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"f1\",\n",
    "#     greater_is_better=True,\n",
    "# )\n",
    "# class MLflowTrainer(Trainer):\n",
    "#     def __init__(self, class_weights_tensor, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.class_weights_tensor = class_weights_tensor\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.get('logits')\n",
    "#         # Compute Focal Loss\n",
    "#         loss = self.focal_loss(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "#     def focal_loss(self, logits, labels, alpha=0.25, gamma=2, reduction='mean'):\n",
    "#         cross_entropy = F.cross_entropy(logits, labels, reduction='none')\n",
    "#         pt = torch.exp(-cross_entropy)\n",
    "#         focal_loss = (alpha * (1 - pt) ** gamma * cross_entropy).mean()\n",
    "#         return focal_loss\n",
    "\n",
    "#     def training_step(self, model, inputs):\n",
    "#         model.train()\n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "#         with self.autocast_smart_context_manager():\n",
    "#             loss = self.compute_loss(model, inputs)\n",
    "#         if self.args.gradient_accumulation_steps > 1:\n",
    "#             loss /= self.args.gradient_accumulation_steps\n",
    "#         self.accelerator.backward(loss)\n",
    "#         if self.args.max_grad_norm is not None:\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), self.args.max_grad_norm)\n",
    "#         logs = {\"loss\": loss.item()}\n",
    "#         self.log(logs)\n",
    "#         return loss.detach()\n",
    "\n",
    "#     def log(self, logs):\n",
    "#         super().log(logs)\n",
    "#         for key, value in logs.items():\n",
    "#             mlflow.log_metric(key, value, step=self.state.global_step)\n",
    "\n",
    "#     def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "#         output = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "#         for key, value in output.items():\n",
    "#             mlflow.log_metric(f\"{metric_key_prefix}_{key}\", value, step=self.state.global_step)\n",
    "\n",
    "# trainer = MLflowTrainer(\n",
    "#     class_weights_tensor=class_weights_tensor,\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=processor,\n",
    "#     data_collator=custom_collate_fn,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
    "# )\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-4 , weight_decay=weight_decay_rate)\n",
    "# num_training_steps = training_args.max_steps\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=int(0.1 * num_training_steps), \n",
    "#     num_training_steps=num_training_steps\n",
    "# )\n",
    "# training_args.lr_scheduler_type = 'linear'\n",
    "# training_args.lr_scheduler_num_warmup_steps = int(0.1 * num_training_steps)  \n",
    "# mlflow.start_run()\n",
    "# trainer.train()\n",
    "# mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d859cd95-5868-4dd1-9ff8-c3e7991273f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855161b-3370-4e63-af5d-36c7bf167c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f42b29-6b5f-4455-9fa5-585825ea76f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cb095-7861-402b-9392-4837e6f4cf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
